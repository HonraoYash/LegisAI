{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  filename                                      text_sections\n",
      "0  BILLS-118hjres119ih.pdf  [IA\\n118TH CONGRESS H. J. RES. 119\\n2D SESSION...\n",
      "1   BILLS-118hr10473ih.pdf  [I\\n118TH CONGRESS H. R. 10473\\n2D SESSION\\nTo...\n",
      "2    BILLS-118hr1165rh.pdf  [IB\\nUnion Calendar No. 673\\n118TH CONGRESS H....\n",
      "3    BILLS-118hr1560ih.pdf  [I\\n118TH CONGRESS H. R. 1560\\n1ST SESSION\\nTo...\n",
      "4    BILLS-118hr1810ih.pdf  [I\\n118TH CONGRESS H. R. 1810\\n1ST SESSION\\nTo...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "\n",
    "# Folder containing PDFs\n",
    "pdf_folder = \"My dataset\"\n",
    "\n",
    "# List to store extracted data\n",
    "data = []\n",
    "\n",
    "# Iterate over all PDF files\n",
    "for filename in os.listdir(pdf_folder):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        file_path = os.path.join(pdf_folder, filename)\n",
    "\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            text_data = []\n",
    "            \n",
    "            # Extract text from each page\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:  # Avoid storing empty pages\n",
    "                    text_data.append(page_text.strip())\n",
    "\n",
    "        # Structure data in dictionary format\n",
    "        extracted_data = {\n",
    "            \"filename\": filename,\n",
    "            \"text_sections\": text_data  # Store as a list of paragraphs\n",
    "        }\n",
    "        \n",
    "        data.append(extracted_data)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save as a JSON file (better than CSV for structured text)\n",
    "df.to_json(\"extracted_text.json\", orient=\"records\", indent=4)\n",
    "\n",
    "# Print the first extracted document\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete. Generated 193 chunks.\n",
      "Sample chunk:\n",
      "IA  118 TH  CON GR ESS  H .  J .  RE S.  119  2 D  S ESSION  Prov iding  for  congressional  disapproval  under  chapter  8  of  title  5 ,  United  States  Code ,  of  the  rule  submitted  by  the  ...\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Enhanced cleaning function with better space handling\"\"\"\n",
    "    # Remove common unwanted patterns\n",
    "    text = re.sub(r'VerDate\\s[\\w\\s]+', '', text)\n",
    "    text = re.sub(r'E:\\\\BILLS\\\\[A-Z0-9.]+', '', text)\n",
    "    text = re.sub(r'Jkt\\s\\d+', '', text)\n",
    "    text = re.sub(r'Frm\\s\\d+\\sFmt\\s\\d+\\sSfmt\\s\\d+', '', text)\n",
    "    text = re.sub(r'\\n+', ' ', text)\n",
    "    text = re.sub(r'^\\d+\\s+', '', text)\n",
    "    \n",
    "    # Normalize spaces - replace multiple spaces with single space\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Fix unicode characters\n",
    "    text = text.replace(\"\\u2018\", \"'\").replace(\"\\u2019\", \"'\")\n",
    "    text = text.replace(\"\\u201c\", '\"').replace(\"\\u201d\", '\"')\n",
    "    text = text.replace(\"\\u2022\", '')\n",
    "    \n",
    "    return text\n",
    "\n",
    "def preprocess_for_gpt(raw_data, max_tokens=1024):\n",
    "    \"\"\"More robust preprocessing pipeline\"\"\"\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    \n",
    "    # Add special tokens for legal domain\n",
    "    special_tokens = [\"[DOC_SEP]\", \"SEC\", \"H.R.\", \"S.\", \"U.S.C.\", \"Fed. Reg.\", \"§\"]\n",
    "    tokenizer.add_special_tokens({\"additional_special_tokens\": special_tokens})\n",
    "    \n",
    "    processed_data = []\n",
    "    \n",
    "    for entry in raw_data:\n",
    "        try:\n",
    "            # Apply cleaning\n",
    "            cleaned_text = clean_text(\" \".join(entry[\"text_sections\"]))\n",
    "            \n",
    "            # Ensure we have valid text\n",
    "            if not cleaned_text or cleaned_text.isspace():\n",
    "                continue\n",
    "                \n",
    "            # Add document separator\n",
    "            formatted_text = f\"{cleaned_text}[DOC_SEP]\"\n",
    "            \n",
    "            # Tokenize and chunk\n",
    "            tokens = tokenizer.tokenize(formatted_text)\n",
    "            \n",
    "            for i in range(0, len(tokens), max_tokens):\n",
    "                chunk = tokens[i:i + max_tokens]\n",
    "                \n",
    "                # Skip chunks that are too short (except last one)\n",
    "                if len(chunk) < 50 and i != 0:\n",
    "                    continue\n",
    "                    \n",
    "                # Convert tokens to text more safely\n",
    "                try:\n",
    "                    chunk_text = tokenizer.convert_tokens_to_string(chunk)\n",
    "                except KeyError:\n",
    "                    # Fallback for problematic tokens\n",
    "                    chunk_text = \" \".join(chunk).replace(\"Ġ\", \" \")\n",
    "                    \n",
    "                processed_data.append({\n",
    "                    \"filename\": entry[\"filename\"],\n",
    "                    \"chunk_id\": f\"{entry['filename']}_{i//max_tokens}\",\n",
    "                    \"text\": chunk_text,\n",
    "                    \"token_count\": len(chunk)\n",
    "                })\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {entry.get('filename', 'unknown')}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return processed_data, tokenizer\n",
    "\n",
    "# Load raw data\n",
    "with open(\"extracted_text.json\", \"r\") as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "# Process data\n",
    "processed_data, tokenizer = preprocess_for_gpt(raw_data)\n",
    "\n",
    "# Save processed data\n",
    "with open(\"gpt_ready_data.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"metadata\": {\n",
    "            \"tokenizer_special_tokens\": tokenizer.additional_special_tokens,\n",
    "            \"max_tokens\": 1024,\n",
    "            \"total_chunks\": len(processed_data)\n",
    "        },\n",
    "        \"data\": processed_data\n",
    "    }, f, indent=4)\n",
    "\n",
    "# Save tokenizer for later use\n",
    "tokenizer.save_pretrained(\"./legal_gpt_tokenizer\")\n",
    "\n",
    "print(f\"Preprocessing complete. Generated {len(processed_data)} chunks.\")\n",
    "if processed_data:\n",
    "    print(\"Sample chunk:\")\n",
    "    print(processed_data[0][\"text\"][:200] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Missing filename!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Validate each entry\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m entry, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing filename!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclean_text\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m entry, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing clean_text!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(entry[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclean_text\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28mstr\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mText must be a string!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Missing filename!"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"gpt_ready_data.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Validate each entry\n",
    "for entry in data:\n",
    "    assert \"filename\" in entry, \"Missing filename!\"\n",
    "    assert \"clean_text\" in entry, \"Missing clean_text!\"\n",
    "    assert isinstance(entry[\"clean_text\"], str), \"Text must be a string!\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
